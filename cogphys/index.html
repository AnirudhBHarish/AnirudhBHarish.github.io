<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CogPhys: Cognitive Load Assessment</title>
    
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background-color: #1a1a1a;
            color: #e0e0e0;
        }
        
        .hero-section {
            background: linear-gradient(135deg, #2563eb 0%, #1e40af 100%);
            color: white;
            padding: 60px 0;
            margin-bottom: 40px;
        }
        
        .hero-section h1 {
            font-size: 2.5rem;
            font-weight: bold;
            margin-bottom: 20px;
        }
        
        .hero-section p {
            font-size: 1.2rem;
            margin-bottom: 30px;
        }
        
        .btn-custom {
            background-color: white;
            color: #2563eb;
            padding: 12px 30px;
            border-radius: 30px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s;
            margin: 5px;
            display: inline-block;
        }
        
        .btn-custom:hover {
            transform: translateY(-3px);
            box-shadow: 0 5px 15px rgba(37, 99, 235, 0.4);
            color: #2563eb;
        }
        
        .section-title {
            font-size: 2rem;
            font-weight: bold;
            margin: 40px 0 30px 0;
            color: #3b82f6;
        }
        
        .abstract-box {
            background-color: #2a2a2a;
            border-left: 4px solid #2563eb;
            padding: 25px;
            margin: 30px 0;
            border-radius: 5px;
        }
        
        .dataset-box {
            background-color: #2a2a2a;
            border-left: 4px solid #2563eb;
            padding: 25px;
            margin: 30px 0;
            border-radius: 5px;
        }
        
        .dataset-box a {
            color: #3b82f6;
            text-decoration: none;
        }
        
        .dataset-box a:hover {
            text-decoration: underline;
        }
        
        .dataset-box ul {
            margin: 15px 0;
        }
        
        .dataset-box .text-muted {
            color: #9ca3af;
        }
        
        .video-side-by-side {
            display: flex;
            gap: 20px;
            margin: 30px 0;
            flex-wrap: wrap;
        }
        
        .video-side-by-side video {
            flex: 1;
            min-width: 300px;
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
        }
        
        .method-image {
            width: 100%;
            max-width: 800px;
            margin: 30px auto;
            display: block;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
        }
        
        .results-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 30px;
            margin: 30px 0;
            max-width: 900px;
        }
        
        .result-item {
            background: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
        }
        
        .result-item img {
            width: 100%;
            height: auto;
            border-radius: 5px;
            margin-bottom: 15px;
        }
        
        .result-item h5 {
            color: #3b82f6;
            margin-bottom: 10px;
        }
        
        .bibtex-box {
            background-color: #2a2a2a;
            border: 1px solid #404040;
            padding: 20px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
        }
        
        .bibtex-box pre {
            color: #e0e0e0;
            margin: 0;
        }
        
        footer {
            background-color: #0a0a0a;
            color: #e0e0e0;
            padding: 20px 0;
            margin-top: 60px;
            text-align: center;
        }
    </style>
</head>
<body>
    
    <!-- Hero Section -->
    <div class="hero-section">
        <div class="container text-center">
            <h1>CogPhys: Assessing Cognitive Load via Multimodal Remote and Contact-based Physiological Sensing</h1>
            <p class="lead">NeurIPS 2025 Datasets and Benchmarks Track</p>
            
            <div class="mt-4">
                <a href="https://openreview.net/forum?id=VJEcCMx16R" class="btn-custom"><i class="fas fa-file-pdf"></i> Paper</a>
                <a href="https://github.com/AnirudhBHarish/CogPhys" class="btn-custom"><i class="fab fa-github"></i> Code</a>
                <a href="https://github.com/AnirudhBHarish/CogPhys" class="btn-custom"><i class="fas fa-chart-bar"></i> Dataset</a>
            </div>
        </div>
    </div>
    
    <!-- Main Content -->
    <div class="container">
        
        <!-- Authors Section -->
        <div class="text-center mb-5">
            <p class="lead">
                <strong>Anirudh Bindiganavale Harish</strong><sup>1*</sup>, 
                <strong>Peikun Guo</strong><sup>1*</sup>, 
                <strong>Bhargav Ghanekar</strong><sup>1**</sup>,
                <strong>Diya Gupta</strong><sup>1**</sup>,
                <strong>Akilesh Rajavenkatanarayan</strong><sup>2</sup>,
                <strong>Manoj Kumar Sharma</strong><sup>2</sup>,
                <strong>Maureen Elizabeth August</strong><sup>2</sup>,
                <strong>Akane Sano</strong><sup>1</sup>,
                <strong>Ashok Veeraraghavan</strong><sup>1</sup>
            </p>
            <p><sup>1</sup>Rice University</p>
            <p><sup>2</sup>General Motors</p>
            <p class="text-muted">
                <small><sup>*</sup>Equal Contribution, <sup>**</sup>Equal Contribution</small>
            </p>
        </div>
        
        <!-- Abstract -->
        <h2 class="section-title">Abstract</h2>
        <div class="abstract-box">
            <p>
                Remote physiological sensing is an evolving area of research. As systems approach clinical precision, there is increasing focus on complex applications such as cognitive state estimation. Hence, there is a need for large datasets that facilitate research into complex downstream tasks such as remote cognitive load estimation. A first-of-its-kind, our paper introduces an open-source multimodal multi-vital sign dataset consisting of concurrent recordings from RGB, NIR (near-infrared), thermal, and RF (radio-frequency) sensors alongside contact-based physiological signals, such as pulse oximeter and chest bands, providing a benchmark for cognitive state assessment. By adopting a multimodal approach to remote health sensing, our dataset and its associated hardware system excel at modeling the complexities of cognitive load. Here, cognitive load is defined as the mental effort exerted during tasks such as reading, memorizing, and solving math problems. By using the NASA-TLX survey, we set personalized thresholds for defining high/low cognitive levels, enabling a more reliable benchmark. Our benchmarking scheme bridges the gap between existing remote sensing strategies and cognitive load estimation techniques by using vital signs (such as photoplethysmography (PPG) and respiratory waveforms) and physiological signals (blink waveforms) as an intermediary. Through this paper, we focus on replacing the need for intrusive contact-based physiological measurements with more user-friendly remote sensors. Our benchmarking demonstrates that multimodal fusion significantly improves remote vital sign estimation, with our fusion model achieving &lt;3 BPM (beats per minute) error for vital sign estimation. For cognitive load classification, the combination of remote PPG, remote respiratory signals, and blink markers achieves 86.49% accuracy, approaching the performance of contact-based sensing (87.5%) and validating the feasibility of non-intrusive cognitive monitoring.
            </p>
        </div>
        
        <!-- Dataset Access -->
        <h2 class="section-title">Dataset Access</h2>
        <div class="dataset-box">
            <p>
                Our multimodal physiological sensing dataset is available for research purposes. 
                Due to the sensitive nature of physiological data, access requires signing a 
                Data Use Agreement (DUA).
            </p>
            <p>
                <strong>To request access to the dataset, please contact:</strong>
            </p>
            <ul>
                <li>Anirudh Bindiganavale Harish: <a href="mailto:anirudhbh@rice.edu">anirudhbh@rice.edu</a></li>
                <li>Ashok Veeraraghavan: <a href="mailto:vashok@rice.edu">vashok@rice.edu</a></li>
            </ul>
            <p class="text-muted">
                <small>Please include your institutional affiliation and intended use case in your request.</small>
            </p>
        </div>

        <!-- Videos Side by Side -->
        <h2 class="section-title">Video Demonstrations</h2>
        <div class="video-side-by-side">
            <video id="video1" controls>
                <source src="videos/tmp_slrec_20251106053231_sp.webm" type="video/webm">
                Your browser does not support the video tag.
            </video>
            <video id="video2" controls>
                <source src="videos/tmp_slrec_20251106053231_sl.webm" type="video/webm">
                Your browser does not support the video tag.
            </video>
        </div>
        
        <!-- Method Overview -->
        <h2 class="section-title">Method Overview</h2>
        <p>
            Our approach combines multimodal remote sensing with contact-based physiological measurements to create a comprehensive benchmark for cognitive load assessment. The system integrates RGB, NIR, thermal, and RF sensors alongside traditional contact-based sensors.
        </p>
        <img src="images/method.png" alt="Method Overview" class="method-image">
        
        <!-- Key Results -->
        <h2 class="section-title">Key Results</h2>
        <div class="results-grid">
            <div class="result-item">
                <img src="images/result1.png" alt="Result 1">
                <h5>Vital Sign Estimation Performance</h5>
                <p>Multimodal fusion achieves &lt;3 BPM error for heart rate estimation.</p>
            </div>
            <div class="result-item">
                <img src="images/result2.png" alt="Result 2">
                <h5>Vital Sign Estimation Performance</h5>
                <p>Multimodal fusion achieves &lt;3 BPM error for respiratory rate estimation.</p>
            </div>
            <div class="result-item">
                <img src="images/result3.png" alt="Result 3">
                <h5>Cognitive Load Classification</h5>
                <p>86.49% accuracy using remote PPG, respiratory signals, and blink markers.</p>
            </div>
        </div>
        
        <!-- BibTeX -->
        <h2 class="section-title">Citation</h2>
        <div class="bibtex-box" style="max-width: 800px; margin: 0 auto;">
<pre>@inproceedings{
harish2025cogphys,
title={CogPhys: Assessing Cognitive Load via Multimodal Remote and Contact-based Physiological Sensing},
author={Anirudh Bindiganavale Harish and Peikun Guo and Bhargav Ghanekar and Diya Gupta and Akilesh Rajavenkatanarayan and MANOJ KUMAR SHARMA and Maureen Elizabeth August and Akane Sano and Ashok Veeraraghavan},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2025},
url={https://openreview.net/forum?id=VJEcCMx16R}
}</pre>
        </div>
        
    </div>
    
    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 Rice University. All rights reserved.</p>
        </div>
    </footer>
    
    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <!-- Synchronized Video Playback -->
    <script>
        const video1 = document.getElementById('video1');
        const video2 = document.getElementById('video2');
        
        // Sync play/pause
        video1.addEventListener('play', () => video2.play());
        video1.addEventListener('pause', () => video2.pause());
        video2.addEventListener('play', () => video1.play());
        video2.addEventListener('pause', () => video1.pause());
        
        // Sync seeking
        video1.addEventListener('seeking', () => {
            if (Math.abs(video1.currentTime - video2.currentTime) > 0.5) {
                video2.currentTime = video1.currentTime;
            }
        });
        video2.addEventListener('seeking', () => {
            if (Math.abs(video2.currentTime - video1.currentTime) > 0.5) {
                video1.currentTime = video2.currentTime;
            }
        });
    </script>
    
</body>
</html>